{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/noahpovis/Desktop/Buildweek22/Build Week 2 /DX.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(i):\n",
    "    i = i.copy()\n",
    "    i.drop_duplicates(subset=['Organization Name','Funding Type'], inplace = True)\n",
    "    i.drop_duplicates(keep='first', inplace= True)\n",
    "    i['Raised Series A'] = np.where(i['Organization Name'].duplicated(), 1, 0)\n",
    "    i.drop_duplicates(subset='Organization Name', keep='last', inplace= True)\n",
    "    i['Money Raised Currency (in USD)'] = pd.to_numeric(i['Money Raised Currency (in USD)'], errors='coerce')\n",
    "    i['Total Funding Amount Currency (in USD)'] = pd.to_numeric(i['Total Funding Amount Currency (in USD)'], errors='coerce')\n",
    "    i['Announced Date'] = pd.to_datetime(i['Announced Date'], errors='coerce')\n",
    "    i['Total Funding Amount Currency (in USD)'] = pd.to_numeric(i['Total Funding Amount Currency (in USD)'], errors='coerce')\n",
    "    i  = i[i['Funding Type'] != 'Series A'] \n",
    "    return i \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = clean(train)\n",
    "val = clean(val)\n",
    "test = clean(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Raised Series A'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape , val.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Investor Names'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_columns(text):\n",
    "    text = text.copy()\n",
    "    # new data frame with split value columns \n",
    "    new = text[\"Organization Industries\"].str.split(\",\", n = 7, expand = True) \n",
    "    # making separate first name column from new data frame \n",
    "    text[\"Primary Industry\"]= new[0] \n",
    "    # making separate last name column from new data frame \n",
    "    text[\"Sub_Ind\"]= new[1] \n",
    "    text[\"Sub_Ind2\"]= new[2]\n",
    "    text[\"Sub_Ind3\"]= new[3]\n",
    "    text[\"Sub_Ind4\"]= new[4]\n",
    "    text[\"Sub_Ind5\"]= new[5]\n",
    "    text[\"Sub_Ind6\"]= new[6]\n",
    "    text[\"Sub_Ind7\"]= new[7]\n",
    "    # Dropping old Name columns \n",
    "    text.drop(columns =[\"Organization Industries\"], inplace = True)\n",
    "\n",
    "    #same thing but for investor column \n",
    "    new1 = text[\"Investor Names\"].str.split(\",\", n = 11, expand = True) \n",
    "    # making separate first name column from new data frame \n",
    "    text[\"Investor1\"]= new1[0] \n",
    "    # making separate last name column from new data frame \n",
    "    text[\"Investor1\"]= new1[1] \n",
    "    text[\"Investor2\"]= new1[2]\n",
    "    text[\"Investor3\"]= new1[3]\n",
    "    text[\"Investor4\"]= new1[4]\n",
    "    text[\"Investor5\"]= new1[5]\n",
    "    text[\"Investor6\"]= new1[6]\n",
    "    text[\"Investor7\"]= new1[7]\n",
    "    text[\"Investor8\"]= new1[8]\n",
    "    text[\"Investor9\"]= new1[9]\n",
    "    text[\"Investor10\"]= new1[10]\n",
    "    text[\"Investor11\"]= new1[11]\n",
    "    # Dropping old Name columns \n",
    "    text.drop(columns =[\"Investor Names\"], inplace = True)\n",
    "    text.drop(columns =[\"Lead Investors\"], inplace = True)\n",
    "    text.drop(columns =[\"Number of Investors\"], inplace = True)\n",
    "    text = text[text['Money Raised'].notna()]\n",
    "    text = text[text['Organization Location'].notna()]\n",
    "    text = text[text['Primary Industry'].notna()]\n",
    "    text = text[text['Money Raised Currency (in USD)'].notna()]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    return text \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = split_columns(train)\n",
    "val = split_columns(val)\n",
    "test = split_columns(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Raised Series A'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Money Raised Currency (in USD)'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 150)\n",
    "text = train[['Organization Description', 'Organization Name']]\n",
    "text = text.sort_index()\n",
    "text= pd.DataFrame(text)\n",
    "\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a first round of text cleaning techniques\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's take a look at the updated text\n",
    "data_clean = pd.DataFrame(text['Organization Description'].apply(round1))\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a second round of cleaning\n",
    "def clean_text_round2(text):\n",
    "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the updated text\n",
    "data_clean = pd.DataFrame(data_clean['Organization Description'].apply(round2))\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['Organization Name'] = train['Organization Name']\n",
    "data_clean.set_index('Organization Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# use this link to start tomorrow: https://stackoverflow.com/questions/37807308/how-can-i-reduce-the-file-size-of-my-ipython-notebook\n",
    "2+2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange data into X features matrix and y target vector\n",
    "target = 'Raised Series A'\n",
    "X_train = train.drop(columns=[target,'tokenized_Description','Announced Date'])\n",
    "y_train = train[target]\n",
    "X_val = val.drop(columns=[target,'tokenized_Description','Announced Date'])\n",
    "y_val = val[target]\n",
    "X_test = test.drop(columns=[target,'tokenized_Description','Announced Date'])\n",
    "y_test = test[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape , y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val['Raised Series A'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(),\n",
    "    SimpleImputer(strategy='median'),\n",
    "    XGBClassifier(n_estimators=100,\n",
    "                  random_state=42,\n",
    "                  n_jobs=-1          \n",
    ")\n",
    ")\n",
    "\n",
    "###scores = cross_val_score(pipeline, X_train, y_train, cv=10, \n",
    "                         ##scoring = 'average_precision')\n",
    "\n",
    "###scores_val = cross_val_score(pipeline, X_val, y_val, cv=10, \n",
    "                         ##scoring = 'average_precision')'''\n",
    "\n",
    "\n",
    "pipe = pipeline.fit(X_train, y_train);\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgb_val = pipe.predict(X_val)\n",
    "print(\"XGBoost Classifier val Recall Accuracy:\", recall_score(y_val, xgb_val))\n",
    "print(\"XGBoost Classifier val Accuracy Score:\", accuracy_score(y_val, xgb_val))\n",
    "print(\"XGBoost Classifier val Precision Accuracy:\", precision_score(y_val, xgb_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# when i ran predict(on X_val, it predicted that 10 of the 18 'Yes' values incorrectly. This could explain why the precision and recall accuracy is so low. \n",
    "unique_elements, counts_elements = np.unique(xgb_val, return_counts=True)\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make another \n",
    "target = 'Raised Series A'\n",
    "X_train = train.drop(columns=[target,'tokenized_Description','Announced Date'])\n",
    "y_train = train[target]\n",
    "X_val = val.drop(columns=[target,'tokenized_Description','Announced Date'])\n",
    "y_val = val[target]\n",
    "X_test = test.drop(columns=[target,'tokenized_Description','Announced Date'])\n",
    "y_test = test[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#install imbalance package\n",
    "pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE \n",
    "sm = SMOTE(random_state = 2) \n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def corpora(d):\n",
    "    #'Dictionary' Representation of all the words in our corpus\n",
    "    #id2word = corpora.Dictionary(d[\"tokenized_Description\"])\n",
    "    #Removing extreme values from the dataset\n",
    "    #id2word.filter_extremes(no_below=5, no_above=0.95)\n",
    "    #bag of words(bow) representation of our corpus\n",
    "    #corpus = [id2word.doc2bow(text) for text in d[\"tokenized_Description\"]]\n",
    "    #return d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = corpora(train)\n",
    "#val = corpora(val)\n",
    "#test = corpora(test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# precision, recall and F1\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "y_train = np.array([number[0] for number in lb.fit_transform(y_train)])\n",
    "\n",
    "recall = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='recall')\n",
    "print('Recall', np.mean(recall), recall)\n",
    "precision = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='precision')\n",
    "print('Precision', np.mean(precision), precision)\n",
    "f1 = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='f1')\n",
    "print('F1', np.mean(f1), f1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitunit2condaf2d4faf30a5848cbbdf1cd45adcc297a",
   "display_name": "Python 3.7.7 64-bit ('unit2': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}